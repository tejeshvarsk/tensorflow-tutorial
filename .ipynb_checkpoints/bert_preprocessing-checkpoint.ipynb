{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5781de44-ed36-48e9-ad0d-e12c1fcfe2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "763ffb69-6d8c-4f49-99d6-b30e9416118d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_a': <tf.Tensor: shape=(), dtype=string, numpy=b'Sponge bob Squarepants is an Avenger'>,\n",
       " 'text_b': <tf.Tensor: shape=(), dtype=string, numpy=b'Barack Obama is the President.'>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = {\n",
    "    \"text_a\": [\n",
    "      \"Sponge bob Squarepants is an Avenger\",\n",
    "      \"Marvel Avengers\"\n",
    "    ],\n",
    "    \"text_b\": [\n",
    "     \"Barack Obama is the President.\",\n",
    "     \"President is the highest office\"\n",
    "  ],\n",
    "}\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
    "\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667a833e-5939-4bbd-b0b4-c30ac310842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_VOCAB = [\n",
    "    # Special tokens\n",
    "    b\"[UNK]\", b\"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\n",
    "    # Suffixes\n",
    "    b\"##ack\", b\"##ama\", b\"##ger\", b\"##gers\", b\"##onge\", b\"##pants\",  b\"##uare\",\n",
    "    b\"##vel\", b\"##ven\", b\"an\", b\"A\", b\"Bar\", b\"Hates\", b\"Mar\", b\"Ob\",\n",
    "    b\"Patrick\", b\"President\", b\"Sp\", b\"Sq\", b\"bob\", b\"box\", b\"has\", b\"highest\",\n",
    "    b\"is\", b\"office\", b\"the\",\n",
    "]\n",
    "\n",
    "_START_TOKEN = _VOCAB.index(b\"[CLS]\")\n",
    "_END_TOKEN = _VOCAB.index(b\"[SEP]\")\n",
    "_MASK_TOKEN = _VOCAB.index(b\"[MASK]\")\n",
    "_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = _VOCAB.index(b\"[UNK]\")\n",
    "_MAX_SEQ_LEN = 8\n",
    "_MAX_PREDICTIONS_PER_BATCH = 5\n",
    "\n",
    "_VOCAB_SIZE = len(_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a51145d-aeec-4b96-87d2-0791e2332053",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(keys=_VOCAB,\n",
    "                                        key_dtype=tf.string,\n",
    "                                        values=tf.range(tf.size(_VOCAB, out_type=tf.int64), dtype=tf.int64),\n",
    "                                        value_dtype=tf.int64),\n",
    "    num_oov_buckets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fcb3662-fc62-4210-9870-eab581d55167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[b'Sp', b'##onge'], [b'bob'], [b'Sq', b'##uare', b'##pants'], [b'is'],\n",
       "  [b'an'], [b'A', b'##ven', b'##ger']]                                  ,\n",
       " [[b'Mar', b'##vel'], [b'A', b'##ven', b'##gers']]]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertTokenizer=text.BertTokenizer(lookup_table, token_out_type=tf.string)\n",
    "bertTokenizer.tokenize(examples['text_a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74ec075f-5e52-40cc-9a5a-a47e92b7b184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[b'Bar', b'##ack'], [b'Ob', b'##ama'], [b'is'], [b'the'], [b'President'],\n",
       "  [b'[UNK]']]                                                              ,\n",
       " [[b'President'], [b'is'], [b'the'], [b'highest'], [b'office']]]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertTokenizer.tokenize(examples['text_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd0f6e87-4a9a-4595-99d3-53b8050db69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.RaggedTensor [[[22, 9], [24], [23, 11, 10], [28], [14], [15, 13, 7]],\n",
       "  [[18, 12], [15, 13, 8]]]>,\n",
       " <tf.RaggedTensor [[[16, 5], [19, 6], [28], [30], [21], [0]], [[21], [28], [30], [27], [29]]]>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertTokenizer=text.BertTokenizer(lookup_table, token_out_type=tf.int64)\n",
    "segment_a = bertTokenizer.tokenize(examples['text_a'])\n",
    "segment_b=bertTokenizer.tokenize(examples['text_b'])\n",
    "segment_a, segment_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecb4e1be-0d3b-484a-a454-8aeac36771e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.RaggedTensor [[22, 9, 24, 23, 11, 10, 28, 14, 15, 13, 7], [18, 12, 15, 13, 8]]>,\n",
       " <tf.RaggedTensor [[16, 5, 19, 6, 28, 30, 21, 0], [21, 28, 30, 27, 29]]>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_a = segment_a.merge_dims(-2,-1)\n",
    "segment_b = segment_b.merge_dims(-2,-1)\n",
    "segment_a, segment_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "071c3d3b-ca01-42e9-b33f-27bfc5c23588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.RaggedTensor [[22, 9, 24, 23],\n",
       "  [18, 12, 15, 13]]>,\n",
       " <tf.RaggedTensor [[16, 5, 19, 6],\n",
       "  [21, 28, 30, 27]]>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer = text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN)\n",
    "trimmed = trimmer.trim([segment_a, segment_b])\n",
    "trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5104c5f-33aa-4da6-b074-ad5576547926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.RaggedTensor [[3, 22, 9, 24, 23, 4, 16, 5, 19, 6, 4],\n",
       "  [3, 18, 12, 15, 13, 4, 21, 28, 30, 27, 4]]>,\n",
       " <tf.RaggedTensor [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_combined, segments_id = text.combine_segments(trimmed, start_of_sequence_id=_START_TOKEN, end_of_segment_id=_END_TOKEN)\n",
    "segments_combined, segments_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1a63aed-c4fe-49af-8409-18b0d06857e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[False, False, True, False, False, False, False, False, False, True,\n",
       "  False],\n",
       " [False, False, True, False, False, False, False, False, True, False,\n",
       "  False]]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_selector = text.RandomItemSelector(max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH, \n",
    "                                         selection_rate=0.2,\n",
    "                                         unselectable_ids=[_START_TOKEN, _END_TOKEN, _UNK_TOKEN])\n",
    "selected=random_selector.get_selection_mask(segments_combined, axis=1)\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "becc9ab7-5b5b-4d24-b731-4b9efdd01673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[17, 1, 1, 1, 23, 1, 1, 1, 1, 1, 19],\n",
       " [1, 1, 20, 1, 1, 1, 1, 1, 1, 1, 1]]>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_values_chooser = text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, mask_token_rate=0.8)\n",
    "mask_values_chooser.get_mask_values(segments_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3086501-e7a6-40d7-9d43-66f8a83065d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.RaggedTensor [[3, 22, 1, 24, 23, 4, 1, 5, 19, 6, 4],\n",
       "  [3, 18, 12, 15, 13, 4, 21, 1, 1, 27, 4]]>,\n",
       " <tf.RaggedTensor [[2, 6],\n",
       "  [7, 8]]>,\n",
       " <tf.RaggedTensor [[9, 16],\n",
       "  [28, 30]]>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_tokens, masked_pos, masked_lm_ids= text.mask_language_model(segments_combined, random_selector, mask_values_chooser)\n",
    "masked_tokens, masked_pos, masked_lm_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f6488a7-379d-4b56-832d-538961e8e32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[CLS]', b'Sp', b'[MASK]', b'bob', b'Sq', b'[SEP]', b'[MASK]',\n",
       "  b'##ack', b'Ob', b'##ama', b'[SEP]'],\n",
       " [b'[CLS]', b'Mar', b'##vel', b'A', b'##ven', b'[SEP]', b'President',\n",
       "  b'[MASK]', b'[MASK]', b'highest', b'[SEP]']]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(_VOCAB, masked_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "667c5713-cb6a-4b86-b411-bf7b4bbe97f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'##onge', b'Bar'],\n",
       " [b'is', b'the']]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(_VOCAB, masked_lm_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b66940c-d4b5-40af-849f-981546bf4537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_word_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[ 3, 22,  1, 24, 23,  4,  1,  5],\n",
       "        [ 3, 18, 12, 15, 13,  4, 21,  1]])>,\n",
       " 'input_mask': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1]])>,\n",
       " 'masked_lm_ids': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[ 9, 16,  0,  0,  0],\n",
       "        [28, 30,  0,  0,  0]])>,\n",
       " 'masked_lm_positions': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[2, 6, 0, 0, 0],\n",
       "        [7, 8, 0, 0, 0]])>,\n",
       " 'masked_lm_weights': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_word_ids, input_mask =  text.pad_model_inputs(masked_tokens, max_seq_length=_MAX_SEQ_LEN)\n",
    "input_type_ids, _ =  text.pad_model_inputs(segments_id, max_seq_length=_MAX_SEQ_LEN)\n",
    "masked_lm_positions, masked_lm_weights = text.pad_model_inputs(masked_pos,max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "masked_lm_ids, _ = text.pad_model_inputs(masked_lm_ids,max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "\n",
    "model_inputs = {\n",
    "    \"input_word_ids\": input_word_ids,\n",
    "    \"input_mask\": input_mask,\n",
    "    \"input_type_ids\": input_type_ids,\n",
    "    \"masked_lm_ids\": masked_lm_ids,\n",
    "    \"masked_lm_positions\": masked_lm_positions,\n",
    "    \"masked_lm_weights\": masked_lm_weights,\n",
    "}\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93cdefe4-28d6-4cc5-87af-da3394c0970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_pretrain_preprocess(vocab_table, feature):\n",
    "    text_a = feature['text_a']\n",
    "    text_b = feature['text_b']\n",
    "    tokenizer = text.BertTokenizer(vocab_table,token_out_type=tf.int64)\n",
    "    segments = [ tokenizer.tokenize(text).merge_dims(-2,-1) for text in [text_a, text_b] ]\n",
    "    trimmer = text.RoundRobinTrimmer(max_seq_length=6)\n",
    "    trimmed_segments = trimmer.trim(segments)\n",
    "    segments_combined, segments_id = text.combine_segments(trimmed_segments,\n",
    "                                                           start_of_sequence_id=_START_TOKEN, \n",
    "                                                           end_of_segment_id=_END_TOKEN)\n",
    "    masked_input_ids, masked_lm_positions, masked_lm_ids = (\n",
    "        text.mask_language_model(segments_combined, random_selector, mask_values_chooser)\n",
    "    )\n",
    "    input_word_ids, input_mask = text.pad_model_inputs(masked_input_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "    input_type_ids, _ = text.pad_model_inputs(segments_id, max_seq_length=_MAX_SEQ_LEN)\n",
    "    masked_lm_positions, masked_lm_weights = text.pad_model_inputs(masked_lm_positions, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    masked_lm_ids,_= text.pad_model_inputs(masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    model ={\n",
    "        \"input_word_ids\":input_word_ids,\n",
    "        \"input_mask\":input_mask,\n",
    "        \"input_type_ids\":input_type_ids,\n",
    "        \"masked_lm_positions\":masked_lm_positions,\n",
    "        \"masked_lm_weights\":masked_lm_weights,\n",
    "        \"masked_lm_ids\":masked_lm_ids\n",
    "    }\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8026592-7029-4eac-adb5-486f5cd5a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(examples).map(functools.partial(bert_pretrain_preprocess, lookup_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09e04c77-a753-46fb-8f5b-10df7d3e2ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_word_ids': <tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[ 3, 22,  1, 24,  4, 16,  1, 19]])>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 0, 0, 0, 0, 1, 1, 1]])>,\n",
       " 'masked_lm_positions': <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[2, 6, 0, 0, 0]])>,\n",
       " 'masked_lm_weights': <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[1, 1, 0, 0, 0]])>,\n",
       " 'masked_lm_ids': <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[9, 5, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63f2e088-c32c-4256-8834-90306653c4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_word_ids': <tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[ 3, 22,  9,  1,  4,  1,  5, 19]])>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 0, 0, 0, 0, 1, 1, 1]])>,\n",
       " 'masked_lm_positions': <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[3, 5, 0, 0, 0]])>,\n",
       " 'masked_lm_weights': <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[1, 1, 0, 0, 0]])>,\n",
       " 'masked_lm_ids': <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[24, 16,  0,  0,  0]])>}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15146de2-3244-4d17-85b5-28d7adba340a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_word_ids': <tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[ 3, 22,  9, 24,  4,  1,  1, 19]])>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 0, 0, 0, 0, 1, 1, 1]])>,\n",
       " 'masked_lm_positions': <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[5, 6, 0, 0, 0]])>,\n",
       " 'masked_lm_weights': <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[1, 1, 0, 0, 0]])>,\n",
       " 'masked_lm_ids': <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[16,  5,  0,  0,  0]])>}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937126ac-0214-46dc-9203-561cff56de0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
