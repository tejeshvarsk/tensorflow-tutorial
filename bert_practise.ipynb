{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e0c343-442b-4e75-a312-5574e669c8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 08:40:05.251429: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-18 08:40:05.354729: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-18 08:40:05.354778: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-18 08:40:05.360225: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-18 08:40:05.385985: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-18 08:40:05.389363: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-18 08:40:07.074003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6fcf9a9-e9b9-405c-82f5-fc79d7bbefbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_a': <tf.Tensor: shape=(), dtype=string, numpy=b'Sponge bob Squarepants is an Avenger'>,\n",
       " 'text_b': <tf.Tensor: shape=(), dtype=string, numpy=b'Barack Obama is the President.'>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = {\n",
    "    \"text_a\": [\n",
    "      \"Sponge bob Squarepants is an Avenger\",\n",
    "      \"Marvel Avengers\"\n",
    "    ],\n",
    "    \"text_b\": [\n",
    "     \"Barack Obama is the President.\",\n",
    "     \"President is the highest office\"\n",
    "  ],\n",
    "}\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5377f6c1-5ee5-497c-b988-d6bcfa89696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_VOCAB = [\n",
    "    # Special tokens\n",
    "    b\"[UNK]\", b\"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\n",
    "    # Suffixes\n",
    "    b\"##ack\", b\"##ama\", b\"##ger\", b\"##gers\", b\"##onge\", b\"##pants\",  b\"##uare\",\n",
    "    b\"##vel\", b\"##ven\", b\"an\", b\"A\", b\"Bar\", b\"Hates\", b\"Mar\", b\"Ob\",\n",
    "    b\"Patrick\", b\"President\", b\"Sp\", b\"Sq\", b\"bob\", b\"box\", b\"has\", b\"highest\",\n",
    "    b\"is\", b\"office\", b\"the\",\n",
    "]\n",
    "\n",
    "_START_TOKEN = _VOCAB.index(b\"[CLS]\")\n",
    "_END_TOKEN = _VOCAB.index(b\"[SEP]\")\n",
    "_MASK_TOKEN = _VOCAB.index(b\"[MASK]\")\n",
    "_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = _VOCAB.index(b\"[UNK]\")\n",
    "_MAX_SEQ_LEN = 8\n",
    "_MAX_PREDICTIONS_PER_BATCH = 5\n",
    "\n",
    "_VOCAB_SIZE = len(_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a91861e-0d8c-4029-9fd6-270ccd348780",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(keys=_VOCAB,\n",
    "                                        key_dtype=tf.string,\n",
    "                                        values=tf.range(tf.size(_VOCAB, out_type=tf.int64), dtype=tf.int64),\n",
    "                                        value_dtype=tf.int64\n",
    "                                       ),\n",
    "    num_oov_buckets=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f30dcb37-6dad-4e6f-abfb-9b5888bb0c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.RaggedTensor [[3, 22, 9, 24, 23, 4, 1, 5, 19, 1, 4],\n",
       "  [3, 1, 12, 1, 13, 4, 21, 28, 30, 27, 4]]>,\n",
       " <tf.RaggedTensor [[6, 9],\n",
       "  [1, 3]]>,\n",
       " <tf.RaggedTensor [[16, 6],\n",
       "  [18, 15]]>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_a = examples['text_a']\n",
    "text_b = examples['text_b']\n",
    "vocab_table=lookup_table\n",
    "tokenizer = text.BertTokenizer(vocab_table, token_out_type=tf.int64)\n",
    "segments = [tokenizer.tokenize(text).merge_dims(-2,-1) for text in [text_a, text_b]]\n",
    "trimmer = text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN)\n",
    "trimmed_segments = trimmer.trim(segments)\n",
    "segments_combined, segments_id = text.combine_segments(trimmed_segments, start_of_sequence_id=_START_TOKEN, end_of_segment_id=_END_TOKEN)\n",
    "segments_combined, segments_id\n",
    "random_selector = text.RandomItemSelector(max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH, selection_rate=0.2,unselectable_ids=[_START_TOKEN, _UNK_TOKEN, _END_TOKEN])\n",
    "mask_values_chooser = text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, mask_token_rate=0.8)\n",
    "input_word_ids, masked_lm_positions, masked_lm_ids = text.mask_language_model(segments_combined, random_selector, mask_values_chooser)\n",
    "input_word_ids, masked_lm_positions, masked_lm_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c4f3482-7850-4319-b7e2-b9119f278c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_pretrain_preprocess(vocab_table, features):\n",
    "    text_a = features['text_a']\n",
    "    text_b = features['text_b']\n",
    "    tokenizer = text.BertTokenizer(vocab_table, token_out_type=tf.int64)\n",
    "    segments = [tokenizer.tokenize(text).merge_dims(-2,-1) for text in [text_a, text_b]]\n",
    "    trimmer = text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN)\n",
    "    trimmed_segments = trimmer.trim(segments)\n",
    "    segments_combined, segments_id = text.combine_segments(trimmed_segments, start_of_sequence_id=_START_TOKEN, end_of_segment_id=_END_TOKEN)\n",
    "    random_selector = text.RandomItemSelector(max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH, selection_rate=0.2,unselectable_ids=[_START_TOKEN, _UNK_TOKEN, _END_TOKEN])\n",
    "    mask_values_chooser = text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, mask_token_rate=0.8)\n",
    "    input_word_ids, masked_lm_positions, masked_lm_ids = text.mask_language_model(segments_combined, random_selector, mask_values_chooser)\n",
    "    input_word_ids, input_mask = text.pad_model_inputs(input_word_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "    input_type_ids, _ = text.pad_model_inputs(segments_id, max_seq_length=_MAX_SEQ_LEN)\n",
    "    masked_lm_positions, masked_lm_weights = text.pad_model_inputs(masked_lm_positions, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    masked_lm_ids, _ = text.pad_model_inputs(masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    model = {\n",
    "        \"input_word_ids\":input_word_ids,\n",
    "        \"input_mask\":input_mask,\n",
    "        \"input_type_ids\":input_type_ids,\n",
    "        \"masked_lm_positions\" : masked_lm_positions,\n",
    "        \"masked_lm_weights\": masked_lm_weights,\n",
    "        \"masked_lm_ids\":masked_lm_ids\n",
    "    }\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a841c20e-1678-4e5c-a79f-a39484e0c28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_word_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[ 3, 22,  9, 24,  1,  4, 16,  5],\n",
       "        [ 3, 18, 12, 15, 13,  4,  1,  1]])>,\n",
       " 'input_mask': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1]])>,\n",
       " 'masked_lm_positions': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[4, 9, 0, 0, 0],\n",
       "        [6, 7, 0, 0, 0]])>,\n",
       " 'masked_lm_weights': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0]])>,\n",
       " 'masked_lm_ids': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[23,  6,  0,  0,  0],\n",
       "        [21, 28,  0,  0,  0]])>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_tensors(examples)\n",
    "    .map(functools.partial(bert_pretrain_preprocess, lookup_table))\n",
    ")\n",
    "\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f6700c-0d66-4dd8-80ea-cdb7c98d1f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
