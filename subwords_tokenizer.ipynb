{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7979fbb2-4b18-4b2c-b1bd-e78b1814d3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 07:28:40.653452: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-19 07:28:40.746896: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-19 07:28:40.746939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-19 07:28:40.751703: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-19 07:28:40.775465: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-19 07:28:40.778546: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-19 07:28:42.495977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b507ab3-b8de-44fa-92ee-6cc17461c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "pwd=pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e831170f-ef04-47c1-94e6-508863c30047",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83f6749-5750-45bc-ad7a-aa7ca8876d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36119c6a-6457-41ef-a45e-a1471adca9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portugese:  e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "English:  and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 07:28:45.242759: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for pt,en in train_examples.take(1):\n",
    "    print(\"Portugese: \", pt.numpy().decode('utf-8'))\n",
    "    print(\"English: \", en.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f104af-e197-4c42-830f-a37bbba531ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = train_examples.map(lambda pt,en : en)\n",
    "train_pt = train_examples.map(lambda pt,en : pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9022787c-7f09-4602-b2eb-da86e3d34a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28dd04ce-bb17-4816-bc50-8c9aa726dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens = [\"[PAD]\",\"[UNK]\",\"[START]\",\"[END]\"]\n",
    "bert_vocab_arguments=dict(\n",
    "    vocab_size=8000,\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    learn_params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51bad3e1-161c-45c8-87c9-313cb7b0b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 35s, sys: 2.15 s, total: 1min 37s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pt_vocab=bert_vocab.bert_vocab_from_dataset(train_pt.batch(1000).prefetch(2), **bert_vocab_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eec55f0-5ce7-4a9d-a7ec-fb2fa2ab71fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
      "['no', 'por', 'mais', 'na', 'eu', 'esta', 'muito', 'isso', 'isto', 'sao']\n",
      "['90', 'desse', 'efeito', 'malaria', 'normalmente', 'palestra', 'recentemente', '##nca', 'bons', 'chave']\n",
      "['##–', '##—', '##‘', '##’', '##“', '##”', '##⁄', '##€', '##♪', '##♫']\n"
     ]
    }
   ],
   "source": [
    "print(pt_vocab[:10])\n",
    "print(pt_vocab[100:110])\n",
    "print(pt_vocab[1000:1010])\n",
    "print(pt_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7131e9d6-acf1-463f-bef8-d25ec917d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath,\"w\") as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f266db82-13ff-4646-997a-c5bfa7a26ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file(\"pt_vocab.txt\", pt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d648c28-122a-4b98-a826-ba06cda713fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 krsethur g680 52382 Mar 18 13:22 vocab.txt\n",
      "-rw-r--r-- 1 krsethur g680 52382 Mar 19 07:22 en_vocab.txt\n",
      "-rw-r--r-- 1 krsethur g680 61124 Mar 19 07:30 pt_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ba10eae-149b-4179-8dd5-eb35e9f1490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 678 ms, total: 1min\n",
      "Wall time: 58.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "en_vocab = bert_vocab.bert_vocab_from_dataset(train_en.batch(1000).prefetch(2), **bert_vocab_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab5bbbfd-966a-40be-ab48-da853e9f0911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
      "['as', 'all', 'at', 'one', 'people', 're', 'like', 'if', 'our', 'from']\n",
      "['choose', 'consider', 'extraordinary', 'focus', 'generation', 'killed', 'patterns', 'putting', 'scientific', 'wait']\n",
      "['##_', '##`', '##ย', '##ร', '##อ', '##–', '##—', '##’', '##♪', '##♫']\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab[:10])\n",
    "print(en_vocab[100:110])\n",
    "print(en_vocab[1000:1010])\n",
    "print(en_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ace68dc-1c43-4117-81ec-1806eaa4c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file(\"en_vocab.txt\", en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd8b9ba-1e91-4b69-ba5f-f9817cf7b04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 krsethur g680 52382 Mar 18 13:22 vocab.txt\n",
      "-rw-r--r-- 1 krsethur g680 61124 Mar 19 07:30 pt_vocab.txt\n",
      "-rw-r--r-- 1 krsethur g680 52382 Mar 19 07:31 en_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c37041b-2753-4bf1-bd57-0d4bb40a664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_tokenizer = text.BertTokenizer(\"pt_vocab.txt\", **bert_tokenizer_params)\n",
    "en_tokenizer = text.BertTokenizer(\"en_vocab.txt\", **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f04f5e0f-c586-49df-92a9-92bb97bde1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .', shape=(), dtype=string)\n",
      "tf.Tensor(b'but what if it were active ?', shape=(), dtype=string)\n",
      "tf.Tensor(b\"but they did n't test for curiosity .\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "    for ex in en_examples:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e742bac-aca6-4ed1-9937-812a0ad32633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15]\n",
      "[87, 90, 107, 76, 129, 1852, 30]\n",
      "[87, 83, 149, 50, 9, 56, 664, 85, 2512, 15]\n"
     ]
    }
   ],
   "source": [
    "token_batch = en_tokenizer.tokenize(en_examples)\n",
    "token_batch = token_batch.merge_dims(-2,-1)\n",
    "\n",
    "for ex in token_batch.to_list():\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e9e1e36-7e8e-421e-bf48-be79e0edefcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'and when you improve search ##ability , you actually take away the one advantage of print , which is s ##ere ##nd ##ip ##ity .',\n",
       "       b'but what if it were active ?',\n",
       "       b\"but they did n ' t test for curiosity .\"], dtype=object)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_tokens = tf.gather(en_vocab,token_batch)\n",
    "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c6d1e3c-1826-4972-b7bf-86d4970f81ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n",
       "       b'but what if it were active ?',\n",
       "       b\"but they did n ' t test for curiosity .\"], dtype=object)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = en_tokenizer.detokenize(token_batch)\n",
    "tf.strings.reduce_join(words, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d2d647b-76a3-4658-a5c9-33ab76ba1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = tf.argmax(tf.constant(reserved_tokens)== \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens)==\"[END]\")\n",
    "\n",
    "def add_start_end(ragged):\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count,1], START)\n",
    "    ends = tf.fill([count,1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "430a361f-88d7-41e6-8a99-340327eef5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'[START] and when you improve searchability , you actually take away the one advantage of print , which is serendipity . [END]',\n",
       "       b'[START] but what if it were active ? [END]',\n",
       "       b\"[START] but they did n ' t test for curiosity . [END]\"],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = en_tokenizer.detokenize(add_start_end(token_batch))\n",
    "tf.strings.reduce_join(words, axis=-1, separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d51cdc4f-7354-4719-867e-29f99b5e4659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    bad_tokens = [re.escape(token) for token in reserved_tokens if token != '[PAD]']\n",
    "    bad_tokens_re = '|'.join(bad_tokens)\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_tokens_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "    return tf.strings.reduce_join(result, axis=-1, separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0049cd8-daf5-420c-9652-101905aa86f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n",
       "       b'but what if it were active ?',\n",
       "       b\"but they did n't test for curiosity .\"], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_examples.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "903d6a34-6c75-4652-9902-883765b543bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'and', b'when', b'you', b'improve', b'searchability', b',', b'you',\n",
       "  b'actually', b'take', b'away', b'the', b'one', b'advantage', b'of',\n",
       "  b'print', b',', b'which', b'is', b'serendipity', b'.']              ,\n",
       " [b'but', b'what', b'if', b'it', b'were', b'active', b'?'],\n",
       " [b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for', b'curiosity',\n",
       "  b'.']                                                                    ]>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_batch = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\n",
    "words = en_tokenizer.detokenize(token_batch)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e75e6dd-9bb8-4e60-b5a0-90beaf0914b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n",
       "       b'but what if it were active ?',\n",
       "       b\"but they did n ' t test for curiosity .\"], dtype=object)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_text(reserved_tokens, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "041ba043-258e-456f-808c-63d7a85e67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, reserved_tokens, vocab_path):\n",
    "        self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "        self.tokenize.get_concrete_function(tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "        self.detokenize.get_concrete_function(tf.TensorSpec(shape=[None,None], dtype=tf.int64))\n",
    "        self.detokenize.get_concrete_function(tf.RaggedTensorSpec(shape=[None,None], dtype=tf.int64))\n",
    "        self.lookup.get_concrete_function(tf.TensorSpec(shape=[None,None], dtype=tf.int64))\n",
    "        self.lookup.get_concrete_function(tf.RaggedTensorSpec(shape=[None,None], dtype=tf.int64))\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "    @tf.function()\n",
    "    def tokenize(self, txt):\n",
    "        tokens = self.tokenizer.tokenize(txt).merge_dims(-2,-1)\n",
    "        return add_start_end(tokens)\n",
    "    @tf.function()\n",
    "    def detokenize(self, tokens):\n",
    "        words = self.tokenizer.detokenize(tokens)\n",
    "        return cleanup_text(self._reserved_tokens, words)\n",
    "    @tf.function()\n",
    "    def lookup(self, tokens):\n",
    "        return tf.gather(self.vocab, tokens)\n",
    "    @tf.function()\n",
    "    def get_vocab_size(self, ):\n",
    "        return self.vocab.shape[0]\n",
    "    @tf.function()\n",
    "    def get_reserved_tokens(self, ):\n",
    "        return tf.constant(self._reserved_tokens)\n",
    "    @tf.function()\n",
    "    def get_vocab_path(self, ):\n",
    "        return self._vocab_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1764fc8d-9200-451a-a632-4bededf1749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.pt = CustomTokenizer(reserved_tokens, \"pt_vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1397ed4-5b86-4a6e-891e-17df430eaf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers.en = CustomTokenizer(reserved_tokens, \"en_vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba6fe4c9-371e-4690-8276-8bbecf36197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
    "tf.saved_model.save(tokenizers,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90746f29-ad57-4753-926e-63563ba85db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=7010>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
    "reloaded_tokenizers.en.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35b16984-3855-49e5-880a-c05d546fbf59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2, 4006, 2358,  687, 1192, 2365,    4,    3]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37ed427d-32a6-4e4f-bd6b-e65759d6775e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[b'[START]', b'hello', b'tens', b'##or', b'##f', b'##low', b'!',\n",
       "        b'[END]']], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = reloaded_tokenizers.en.lookup(tokens)\n",
    "text_tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d6933d2-6427-409a-be39-54e44e7f1581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'hello tensorflow !'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_trip = reloaded_tokenizers.en.detokenize(tokens)\n",
    "round_trip.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4d03900-87b6-4337-9d08-120dd70062a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: ted_hrlr_translate_pt_en_converter/ (stored 0%)\n",
      "  adding: ted_hrlr_translate_pt_en_converter/fingerprint.pb (stored 0%)\n",
      "  adding: ted_hrlr_translate_pt_en_converter/variables/ (stored 0%)\n",
      "  adding: ted_hrlr_translate_pt_en_converter/variables/variables.data-00000-of-00001 (deflated 51%)\n",
      "  adding: ted_hrlr_translate_pt_en_converter/variables/variables.index (deflated 33%)\n",
      "  adding: ted_hrlr_translate_pt_en_converter/saved_model.pb (deflated 91%)\n",
      "  adding: ted_hrlr_translate_pt_en_converter/assets/ (stored 0%)\n",
      "  adding: ted_hrlr_translate_pt_en_converter/assets/en_vocab.txt (deflated 54%)\n",
      "  adding: ted_hrlr_translate_pt_en_converter/assets/pt_vocab.txt (deflated 57%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r {model_name}.zip {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2c87f62-3741-4075-a0d5-8c74c3db4ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172K\tted_hrlr_translate_pt_en_converter.zip\n"
     ]
    }
   ],
   "source": [
    "!du -h *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c3c8290-2cd7-43af-921d-dbb02ee4c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_lookup = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.TextFileInitializer(filename='pt_vocab.txt', \n",
    "                                  key_dtype=tf.string, \n",
    "                                  key_index=tf.lookup.TextFileIndex.WHOLE_LINE, \n",
    "                                  value_dtype=tf.int64, \n",
    "                                  value_index=tf.lookup.TextFileIndex.LINE_NUMBER),\n",
    "    num_oov_buckets=1)\n",
    "pt_tokenizer = text.BertTokenizer(pt_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0bffc445-b858-409d-b8ad-89676d42c788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([7765,   85,   86,   87, 7765])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_lookup.lookup(tf.constant(['é', 'um', 'uma', 'para', 'não']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c0607-1ff7-4c83-ac2f-796d8025bb21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
