{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f4cf7b-9e1a-4f70-be0f-06f415a133fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651b1e14-8d60-4083-b1ed-9260906b83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bedad091-577a-4e3f-a86c-1f4c7dae1400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text : 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text= open(path_to_file, \"rb\").read().decode(encoding='utf-8')\n",
    "print(f'Length of text : {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c62b047-3910-4149-a7cd-c3794af8f695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b89aba8-0c36-460d-96ce-71332a42097e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829ae8c0-5030-4b36-93b2-840ce19e1824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "chars=tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58b2a14e-9b1c-4da6-918d-ff830b476813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=None)\n",
    "ids=ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc588cc1-89b3-426d-880a-6b857a29b69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=None, invert=True)\n",
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd10f733-8633-4780-a7af-8494eb28565b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad3506fd-1137-451a-ae81-3f1f2bcadd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983c6e8a-d50a-4d16-9abf-24d405de11d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'abcdefg', b'xyz'], dtype=object)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_from_ids(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51cfe680-1eba-425f-bbbf-73a7bb3ac0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, input_encoding='UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "946c708c-d92c-461e-a81d-cb555cbda649",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae472e6e-bffd-4d0e-b863-daf3e87fb7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17379046-5f02-4478-9b55-887ad578b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c57dff04-6c41-4af6-a2cd-7966ef577dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56d1d416-7506-493d-8adc-0a7a07b6bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ceb8a25-efda-4e11-98b8-66225e9f2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cf8ca4d-0b43-4f8c-a83e-1b1b46ac0c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list('Tensorflow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57e5ec9f-5b9d-4e2e-953c-a0d4dca96812",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b34803b-10bc-4065-9d79-d56008594387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Example : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target Example : b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(f'Input Example : {text_from_ids(input_example)}')\n",
    "    print(f'Target Example : {text_from_ids(target_example)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecd731cc-b4a7-4702-a0e1-64272518b444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE=64\n",
    "BUFFER_SIZE=10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9b4ad85-49fc-4fea-b939-87e9c220e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(ids_from_chars.get_vocabulary())\n",
    "embedding_dim=256\n",
    "rnn_units=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3453bcb-7b55-413f-8dd8-1ce50abf2175",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units, return_state=True, return_sequences=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x=inputs\n",
    "        x= self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x,states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0467275-229d-4f61-bf5f-e68564c18ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbcc2dfc-4b1b-4114-bce9-967db7530f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions, states = model(input_example_batch, return_state=True)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "410a9f45-5c43-4c3a-bf4f-bb1b4aa19501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4022850 (15.35 MB)\n",
      "Trainable params: 4022850 (15.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab6c557e-b070-4815-81a5-07aa3dbd2e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 66), dtype=float32, numpy=\n",
       "array([[-0.00091549,  0.00989263, -0.01140929, ...,  0.01418819,\n",
       "         0.00300857,  0.0048025 ],\n",
       "       [ 0.00328853,  0.00044164, -0.01417324, ...,  0.00158249,\n",
       "        -0.00413412, -0.00527469],\n",
       "       [-0.00141547,  0.00301661, -0.00258737, ..., -0.00362638,\n",
       "        -0.01150018,  0.00934668],\n",
       "       ...,\n",
       "       [-0.00756153, -0.00038743, -0.00720855, ..., -0.00726603,\n",
       "         0.01039174, -0.00569758],\n",
       "       [-0.00862981,  0.0017312 , -0.00678661, ..., -0.00501955,\n",
       "        -0.00461998, -0.00591494],\n",
       "       [-0.0045813 , -0.00521695,  0.01091945, ..., -0.00926325,\n",
       "        -0.0070805 , -0.00677072]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "815bcba3-26e3-44b1-a525-edecb0647f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
       "array([19,  9,  8,  1, 52, 21, 52, 24, 21, 15, 32, 35, 38, 41, 30, 58, 21,\n",
       "       44, 19, 12, 58, 45, 11, 21, 55,  6, 54, 17,  7, 61, 58, 19,  8, 65,\n",
       "       63, 60, 24, 15, 28, 25, 60,  7, 61, 40, 37, 29, 42, 45, 48, 11, 49,\n",
       "       30, 11,  6, 52, 19, 43, 34,  6, 46, 14, 41, 32, 63, 18,  0, 13,  6,\n",
       "       31,  3,  5, 30, 53, 40, 12, 19, 54, 61, 37,  5, 29, 59, 17,  0, 57,\n",
       "        0, 41,  0,  0, 20, 33,  4, 23, 57,  9, 18, 13, 13, 36, 48])>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1)\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3329528a-5213-4e8d-b613-330fcc7e0e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'ULINA:\\nNay, rather, good my lords, be second to me:\\nFear you his tyrannous passion more, alas,\\nThan '\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"F.-\\nmHmKHBSVYbQsHeF;sf:Hp'oD,vsF-zxuKBOLu,vaXPcfi:jQ:'mFdU'gAbSxE[UNK]?'R!&Qna;FovX&PtD[UNK]r[UNK]b[UNK][UNK]GT$Jr.E??Wi\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2196c2fb-8895-4bf6-9403-768c9fdcfcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1901307, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15e1b17a-76ab-4218-804e-ddb7816f535b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.03142"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "726187a2-c66c-4ec3-a402-d50337a8c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60a4149f-108c-4fd4-bdf6-9a01398ece52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 122s 701ms/step - loss: 2.6856\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 122s 704ms/step - loss: 1.9667\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 123s 710ms/step - loss: 1.6921\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 122s 705ms/step - loss: 1.5348\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 123s 714ms/step - loss: 1.4394\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 126s 727ms/step - loss: 1.3734\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 123s 711ms/step - loss: 1.3225\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 119s 686ms/step - loss: 1.2773\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 121s 699ms/step - loss: 1.2363\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 121s 698ms/step - loss: 1.1963\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 122s 703ms/step - loss: 1.1572\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 123s 710ms/step - loss: 1.1155\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 126s 727ms/step - loss: 1.0705\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 127s 737ms/step - loss: 1.0241\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 129s 744ms/step - loss: 0.9751\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 126s 727ms/step - loss: 0.9235\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 129s 744ms/step - loss: 0.8723\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 130s 753ms/step - loss: 0.8205\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 130s 755ms/step - loss: 0.7696\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 131s 757ms/step - loss: 0.7242\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77a34d0d-e410-4380-be9c-3950a005d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3f619d7-224c-48c6-885e-acc779e2e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79994814-4811-46c7-b333-34a670d8f1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Then let the feast within me for I'll swear,\n",
      "Yet art thou not change your cause. Pray you, sir, where go smell.\n",
      "\n",
      "GRUMIO:\n",
      "What, myself have spoke her from hencefts being their\n",
      "Arm use her time?\n",
      "\n",
      "BRUTUS:\n",
      "Come, farewell.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Nay, good my lord; and what stir\n",
      "King Henry he is kindly.'\n",
      "\n",
      "LUCINTIO:\n",
      "Were him her mine. If you will bear with palk for hands\n",
      "The worst is full of poisonous,--\n",
      "\n",
      "MENENIUS:\n",
      "They could not dread to harm. I lock his friend,\n",
      "That may be hep upon your servant; hence fix\n",
      "themselves lasts.\n",
      "\n",
      "PETRUCHIO:\n",
      "Come on, my fortunes to his livery\n",
      "on thee I may knock. But what a\n",
      "man is hanged bill'd falsely than an inhorce my daughter\n",
      "great once, for all the sun that will from thee thee Dike of\n",
      "That quench my pack fair back-words.\n",
      "\n",
      "PETRUCHIO:\n",
      "Pompey?\n",
      "\n",
      "All:\n",
      "This, that he die, present the state\n",
      "To right your valour, with his princely sellcess\n",
      "So many milthough abidle stones;\n",
      "And what your highness are the embracements of mine;\n",
      "Which, though nothing she dreads of steep; your br \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.5887868404388428\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f68cbe73-4cec-4c52-854b-2a490c073862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nThe queen as in a pastly sir, and, for the\\nhour all's rock, the duke is my sudge done.\\n\\nQUEEN MARGARET:\\nPoor Clifford, then? who can say he said 'Gains?\\n\\nFirst Citizen:\\nYourself since fair before thy tale.\\n\\nMURCIUS:\\n'Tis good to help met with a virtuous,\\nand as I please my hopes, and no looks look intogest\\nBy undined browled and up with angel:\\nbe singly cloud convey by the year;\\nBut now I'll ruin of his their power.\\n\\nEDWARD:\\nSay, each in my made, here will I rele thy death,\\nNo fault on evidens in his passage.\\n\\nLUCIO:\\n\\nProvost:\\nI can better be before thy hand. This I that sin your\\ncass.\\n\\nFirst Senator:\\nYou are so; and our fine robes.\\n\\nLUCENTIO:\\nAnd I'll be resolute: then at Paulina's heart I woo'd;\\nAnd, paity, give shall in the uteemories,\\nLest blood will follow thine enemy: speak, cannot\\nplace cuttom, forbear: I make so permit a pox o' the present,\\nPrefare the admoning of their prodance,\\nOr forbid or round: and here he ceased\\nto get the purpositious,\\nLadies agon him.\\n\\nFirst Senator:\\nN\"\n",
      " b\"ROMEO:\\n\\nProvost:\\nHast thou not swear this luttle and the rest make me\\nTurner him hence? I am gone, in the sud ridg'd him as sen\\nAssale now. Sweet Cliff! go with me; and the\\nbeggar then for both of them knock.\\n\\nYORK:\\nSince then, since we are on it stops his house: give me\\ncouncile to my hid enforce thy thorn:\\nWho goes error all. I say to the poor duke\\naccess to twenty piece of mocks. Had he for me\\nIn hollow partners approach by the\\nAspectal'd bodies of my subject?\\n\\nNORTHUMBERLAND:\\nNo, Pausin, earl of waters, underead, it is\\nMy mind of blanks do with him and report death.\\n\\nBIANCA:\\nInfer a feather from her news: they shall set them now,\\nWere on his mouth, our pale, achieved withal.\\nAdvance his person more, onem abroad\\nWas in my feeble, who never speak\\nTo the purgess; but then every one\\nBalingbroke within Stiff and his majesty.\\n\\nGLOUCESTER:\\nI am Tyrrel, so as the sopt along;\\nThis fearing load, we see the winds within the\\nextremity.\\n\\nGREEN:\\nShe's dead, or, between these kings, and all undone\\nThat\"\n",
      " b\"ROMEO:\\nShe knew stwasing Fortune badded lends wither.\\n\\nThird Citizen:\\nYour chaneman, so hot, my desient,\\nOne gown instruction, that he scock thee from thy place.\\nGo, play at, good citizen:\\nI warrant thee let's away; the great their\\nshould-pack open guilt or water: no strength, reverself kiss, and makes all read\\nAnd leave our putienger to a king, by him,\\nPoor hopses places of knew our strokes, as bittenly\\nOne past them on their tender days, and the\\nporter, yet with us as from your eye;\\nHow many thou durst twenty years, by course of York.\\n\\nPRINCE EDWARD:\\nMy rights are not ignoble; some\\ncrase, my blood.\\n\\nCORIOLANUS:\\nMaparation! What would you say,\\nTickled with grief of heaven brings; their powerful of Right Prizen\\nCannot be current in soft: so were you comfort\\nThe patricians, shall be deliver'd?\\nWhat works thou to Hast thmone advertest!\\n\\nKING RICHARD II:\\nForthwith unto my people by remember\\nWhose double enemies the eyes wither.\\n\\nESCALUS:\\nSee how I will not old Aybiur's full of delf\\nWith nothing\"\n",
      " b\"ROMEO:\\nThou didst promise he equall'd; and mark how makes\\nWith all enemies and dost noble lord.\\n\\nPRINCE:\\nLet's meet her manner there were his own\\nbright famous; for the world's instruction\\nThe view of our franch of your combians,\\nThey should be golden between your life,\\nSmail your met off away to-day.\\n\\nROMEO:\\nWho all for Titus Lartius artitor, that\\nfear to them; the one begin of God before you good.\\n\\nQUEEN ELIZABETH:\\nFarewell, a horse!' whose hasty welcome!\\nLiett Juliet is mann'd, my place, some wooency!\\n\\nDERBY:\\nGod kill my uncle Yir day with him.\\n\\nWARWICK:\\nSweet sickly, like no blots; she who shall see, dear God!\\nShe'll not good sweet Kate; it shall become my father,\\nThat one speaks of all, whose dewer calls\\nAnd in the sun extil upon her grave;\\nAnd he to be as counterful meets,\\n'Whoolest thou in this dearth, you from me this must make\\nWill have this curtary to the sight; the king perform'd\\nWith silengians five peace impediment, would have this\\ncarted in them, but thou sits I have discharred\"\n",
      " b\"ROMEO:\\nNow, belike, Buckingham, if he dwells; he reasons, spare hair,\\nYour leciors have ever arm'd for his quale. But, O them!\\nLet me fall would go watch you as fast;\\nMight there be not comforted in my popular\\nOnce his waying, this only in his bed,\\nAnd what you cast upon thee, tender men,\\nWith place of open and reverence\\nScars, they shall be wed: ere we fear'd up\\n'T: then shortly seemett the loss of Kings and tribunes,\\nAnd a dread charge he cried to mine, unless\\nA whoeverved in thy heir of those the\\ncountive, and he shall be continent\\nto sit to carvel about time reigns,\\nI'll frowndal II; O, it is thought the daughter of a king\\nFor make me living your enemy is less?\\n\\nGLOUCESTER:\\n\\nGLOUCESTER:\\nYet thus my soul return; my conscience,\\nEight in tute the thing Aufidius Lord Angelo\\n\\nTYBALT:\\nWhy, how comes the holy serves to be true.\\n\\nFirst Citizen:\\nBe elonged, my lord, we see the way?\\n\\nVALERIA:\\nMore well where I may net answer as it\\nMy liege, this blood Duke of Burnient pale.\\n\\nJOHN OF GAUNT:\\nSir, th\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.99166202545166\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc1264d2-9502-4585-bcc2-d9a31f675646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d79387-e387-4377-86d0-ecb95bbec840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
